hydra_run_dir: logs/${now:%Y-%m-%d}/${now:%H-%M-%S}

hydra:
    run:
        dir: ${hydra_run_dir}

paths:
    dataset_root: data/ade20k
    train_images: images/train
    train_masks: masks/train
    val_images: images/val
    val_masks: masks/val

models:
    segformer:
        variant:
            b0:
                huggingface_name: nvidia/segformer-b0-finetuned-ade-512-512
                image_size: 512
            b2:
                huggingface_name: nvidia/segformer-b2-finetuned-ade-512-512
                image_size: 512
            b5:
                huggingface_name: nvidia/segformer-b5-finetuned-ade-640-640
                image_size: 640

dataset:
    name: ade20k
    num_classes: 150
    ignore_index: 255

dataloader:
    train:
        batch_size: 4
        shuffle: true
        num_workers: 4
        pin_memory: true
        drop_last: false
    val:
        batch_size: 4
        shuffle: false
        num_workers: 4
        pin_memory: true
        drop_last: false

metrics:
    pixel_accuracy:
        num_classes: ${dataset.num_classes}
        ignore_index: ${dataset.ignore_index}
    jaccard_index:
        num_classes: ${dataset.num_classes}
        ignore_index: ${dataset.ignore_index}

lit_wrapper:
    segformer:
        model_name: segformer
        pretrained_name: ${models.segformer.variant.b5.huggingface_name}
        num_classes: ${dataset.num_classes}
        ignore_index: ${dataset.ignore_index}
        learning_rate: 0.00006
        weight_decay: 0.01

trainer:
    max_epochs: 100
    precision: 16
    log_every_n_steps: 2
    enable_checkpointing: true
    enable_progress_bar: true
    enable_model_summary: false
    default_root_dir: ${hydra_run_dir}
    callbacks:
        model_checkpoint:
            monitor: val_mean_iou
            mode: max
            save_top_k: 3
            filename: segformer-{epoch:02d}-{val_mean_iou:.4f}

optimizer:
    name: AdamW
    lr: ${lit_wrapper.segformer.learning_rate}
    weight_decay: ${lit_wrapper.segformer.weight_decay}
scheduler:
    name: PolyLR
    max_epochs: ${trainer.max_epochs}
    power: 0.9
